## 학습이란?
* train 데이터로부터 가중치 W를 최적의 값으로 얻는 것
* Loss 함수를 최소로 만드는 W 찾기
* 함수의 기울기를 활용한 경사법

### 참고
* 직선으로 분류 가능하면(선형 분리 가능) 데이터로부터 자동으로 학습 가능 => 유한 번의 학습을 통해 가능 (퍼셉트론 수렴 정리)
* 비선형 분리는 자동으로 학습 불가능
* __신경망과 딥러닝은 기존 머신러닝보다 사람 개입 줄임__

***

### 1) 머신러닝
* 이미지의 feature(특징)은 보통 벡터로 기술
* 컴퓨터비전 분야에선 HOG, SIFT, SURF 등의 feature 사용
* 이미지 데이터 -> feature 추출(Hog, SIFT, SURF 등) -> 벡터로 변환 -> 학습(SVM, KNN 등) -> 결과
* `feature`는 `사람`이 설계함 (무슨 특징을 추출할지)

### 2) 딥러닝
* end-to-end machine learning
* __사람이 아예 개입 안함__ (이미지를 있는 그대로 학습, `feature`도 `기계`가 학습)
* 이미지 데이터 -> 신경망(딥러닝) -> 결과 

***

## 엔트로피
* 불확실 정도
* __h(x) = -log*IDF(x)__
* https://ohyez.tistory.com/3
### IDF : 정보량
* ex) 주사위를 던져서 짝수일 x의 정보량 = IDF(x) = 1/2 = I
* 엔트로피 = 평균정보량

***

## 수치미분
* 수치 미분 : 아주 작은 차분으로 미분하는 것 (근사치)
* 해석적(analytic) 미분 : 수식으로 미분하는 것 (오차 X)

## 기울기
- 모든 변수의 편미분을 벡터로 정리한 것
- __기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다!!!__
- 안장점 : 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점

## 경사법
- 기울기가 0이 되는 장소를 찾음, 반드시 최소값을 보장 X (극솟값이나 안장점일 가능성 있음)
- 고원(plateau, 플래토): 평평한 곳으로 파고들면서 학습이 진행되지 않는 정체기
- 최솟값이 되는 장소를 찾는 문제에서 기울기를 단서로 방향 정함
- __경사법 : 기울어진 방향으로 이동 반복, 함수의 값을 점차 줄이는 것__
- `경사 하강법(gradient descent method)`: 최솟값을 찾는 것 
- `경사 상승법(gradient ascent method)`: 최댓값을 찾는 것
	+ 사실 딥러닝에선 손실함수 부호만 바꿔서 둘다 사용 가능한데, 보통 하강법으로 등장함
- 하이퍼 파라미터 : 학습률같이 사람이 직접 지정하는 파라미터

## 신경망 학습 절차
- 전제 : weight, bias는 적응 가능, train data에 적응하도록 조정하는 과정이 학습이다.
1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴, 이 미니배치의 손실 함수 값을 줄이는 것을 목표로 함
2. 기울기 산출 : 미니배치의 손실함수 값을 줄이기 위해 각 weight 매개변수의 기울기를 구함
3. 매개변수 갱신 : weight 매개변수를 기울기 방향으로 아주 조금 갱신
4. 1~3단계 반복
