## 매개변수 갱신
- `최적화`: 매개변수의 최적값을 찾는 문제
- `확률적 경사 하강법(SGD)`: 매개변수의 기울기를 구해 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복해서 최적의 값에 다가감
	+ 단점 : __`비등방성(anisotropy) 함수`에서 탐색 경로가 비효율적__
		- 비등방성 함수 : 방향에따라 성질(여기서는 기울기)이 달라지는 함수
- SGD 단점 개선 : `모멘텀`, `AdaGrad`, `Adam`

***

## 모멘텀(Momentum)
- 속도: 기울기 방향으로 힘을 받아 물체가 가속된다는 물리법칙
- `αv`: 물체가 아무런 힘을 받지 않을 때 서서히 하강시킴

## AddGrad
- `학습률 감소 (learning rate decray)`
	- 학습을 진행하면서 학습률을 점차 줄여가는 방법
	- 처음에는 크게 학습하다가 조금씩 작게 학습하는 것
	- 간단한 방법은 `매개변수 전체`의 학습률을 일괄적으로 낮추는 것
- __`AddGrad`는 각각의 매개변수에 맞게 학습률을 낮춰줌__(adaptive)
- `h` : 기울기 값을 제곱하여 계속 더해줌
- 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아짐 (학습률 감소가 매개변수의 원소마다 다르게 적용됨)
- 무한히 학습한다면 어느 순간 갱신량이 0이 됨
	+ 이 문제 개선한 것이 `RMSProp`
		- `지수이동평균(EMA)`: 과거의 모든 기울기를 균일하게 더해가는 것(AddGrad)이 아니라 먼 과거의 기울기는 잊고 새로운 기울기 정보를 크게 반영함 (과거 기울기의 반영 규모를 기하급수적으로 감소시킴)

## Adam
