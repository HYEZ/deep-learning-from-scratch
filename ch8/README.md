## 딥러닝
- 딥러닝은 층을 깊게 한 심층 신경망이다.

### 정확도 높이기
- 앙상블 학습, 학습률 감소(learning rate decray), 데이터 확장(data augmentation)
- Data augmentation
	- 입력이미지를 알고리즘을 동원해 인위적으로 확장
	- `회전`, `이동`, `crop`, `flip(좌우뒤집기)` 등 미세한 변화 주어서 이미지 개수 늘림
	- 데이터가 몇 개 없을 때 효과적

### 층을 깊게 하는 이유
1. 깊게 할 수록 신경망의 매개변수 수가 줄어든다.
	- 매개 변수 수를 줄여 __넓은 수용 영역(국소적인 공간 영역) 확보__
	- 활성화 함수를 합성곱 계층에 끼움 => 활성화 함수가 `비선형 힘`을 가하고 비선형 함수가 겹치면서 더 복잡한 것도 표현 가능
2. 학습의 효율성 증가
	- 적은 학습 데이터로 효율적으로 학습 가능
	- 학습해야 할 문제를 `계층적으로 분해` 가능
		- ex) 처음 층은 에지(edge) 학습에 전념(개가 등장하는 이미지보다 에지가 등장하는 이미지가 많고, 에지의 패턴은 개라는 패턴의 구조보다 훨씬 간단하기 때문)
	- 층을 깊이 함으로써 각 층이 학습해야할 문제를 `풀기 쉬운 단순한 문제`로 분해

***

## GoogLeNet
- 인셉션 구조 
	- 가로방향에 `폭`이 있는 것
	- 크기가 다른 필터를 여러 개 적용 후 그 결과를 결합
- 인셉션 구조를 하나의 빌딩 블록(구성 요소)으로 사용하는 것
- 1x1 필터를 사용한 합성곱 계층을 많은 곳에서 사용 
	- 채널 쪽으로 크기를 줄이는 것
	- 매개변수 제거와 고속 처리에 기여

## ResNet
- 마이크로소프트 팀이 개발
- `스킵 연결` 도입
	- 딥러닝에서 층이 너무 깊으면 오히려 성능 떨어지는 문제 해결
	- 층의 깊이에 비례해 성능을 향상시킬 수 있음
		- 역전파 때 스킵 연결이 `신호 감쇠`를 막아주기 때문
		- 역전파 때 상류의 기울기를 하류에 그대로 흘려보냄
		- 스킵 연결로 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 __의미 있는 기울기__ 가 전해지리라 기대 가능
		- 층을 깊게 할수록 기울기가 작아지는 소실 문제를 스킵 연결이 해결함
	- 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더함

## 전이 학습(transfer learning)
- 이미 학습된 가중치를 다른 신경망에 복사(가중치 초기값으로 학습된 가중치 사용)하고 그 상태로 새로운 데이터셋으로 재학습(fine tuning) 수행
- 보유한 데이터셋이 적을 때 유용

## 딥러닝 고속화
- 합성곱 계층에서 이루어지는 연산을 어떻게 고속으로 효율적으로 하느냐

### GPU(Graphical Processing Uint) 사용
- 병렬 수치 연산 고속으로 처리 가능
- GPU는 대량 병렬 연산(큰 행렬의 내적 등)에 특화, CPU는 연속적인 복합 계산에 특화
- 엔디비아 GPU 
	- GPU 컴퓨터 통합 개발 환경인 CUDA 사용
	



