## 딥러닝
- 딥러닝은 층을 깊게 한 심층 신경망이다.

### 정확도 높이기
- 앙상블 학습, 학습률 감소(learning rate decray), 데이터 확장(data augmentation)
- Data augmentation
	- 입력이미지를 알고리즘을 동원해 인위적으로 확장
	- `회전`, `이동`, `crop`, `flip(좌우뒤집기)` 등 미세한 변화 주어서 이미지 개수 늘림
	- 데이터가 몇 개 없을 때 효과적

### 층을 깊게 하는 이유
1. 깊게 할 수록 신경망의 매개변수 수가 줄어든다.
	- 매개 변수 수를 줄여 __넓은 수용 영역(국소적인 공간 영역) 확보__
	- 활성화 함수를 합성곱 계층에 끼움 => 활성화 함수가 `비선형 힘`을 가하고 비선형 함수가 겹치면서 더 복잡한 것도 표현 가능
2. 학습의 효율성 증가
	- 적은 학습 데이터로 효율적으로 학습 가능
	- 학습해야 할 문제를 `계층적으로 분해` 가능
		- ex) 처음 층은 에지(edge) 학습에 전념(개가 등장하는 이미지보다 에지가 등장하는 이미지가 많고, 에지의 패턴은 개라는 패턴의 구조보다 훨씬 간단하기 때문)
	- 층을 깊이 함으로써 각 층이 학습해야할 문제를 `풀기 쉬운 단순한 문제`로 분해

***

## GoogLeNet
- 인셉션 구조 
	- 가로방향에 `폭`이 있는 것
	- 크기가 다른 필터를 여러 개 적용 후 그 결과를 결합
- 인셉션 구조를 하나의 빌딩 블록(구성 요소)으로 사용하는 것
- 1x1 필터를 사용한 합성곱 계층을 많은 곳에서 사용 
	- 채널 쪽으로 크기를 줄이는 것
	- 매개변수 제거와 고속 처리에 기여

## ResNet
- 마이크로소프트 팀이 개발
- `스킵 연결` 도입
	- 딥러닝에서 층이 너무 깊으면 오히려 성능 떨어지는 문제 해결
	- 층의 깊이에 비례해 성능을 향상시킬 수 있음
		- 역전파 때 스킵 연결이 `신호 감쇠`를 막아주기 때문
		- 역전파 때 상류의 기울기를 하류에 그대로 흘려보냄
		- 스킵 연결로 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 __의미 있는 기울기__ 가 전해지리라 기대 가능
		- 층을 깊게 할수록 기울기가 작아지는 소실 문제를 스킵 연결이 해결함
	- 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더함

## 전이 학습(transfer learning)
- 이미 학습된 가중치를 다른 신경망에 복사(가중치 초기값으로 학습된 가중치 사용)하고 그 상태로 새로운 데이터셋으로 재학습(fine tuning) 수행
- 보유한 데이터셋이 적을 때 유용

*** 

## 딥러닝 고속화
- 합성곱 계층에서 이루어지는 연산을 어떻게 고속으로 효율적으로 하느냐

### GPU(Graphical Processing Uint) 사용
- 병렬 수치 연산 고속으로 처리 가능
- GPU는 대량 병렬 연산(큰 행렬의 내적 등)에 특화, CPU는 연속적인 복합 계산에 특화
- 엔디비아 GPU 
	- GPU 컴퓨터 통합 개발 환경인 CUDA 사용

### 분산 학습
- 다수의 GPU와 기기로 계산을 분산
	- 분산 학습을 지원하는 딥러닝 프레임워크 이용
		- 구글의 텐서플로우와 마이크로 소프트의 CNTK
- 컴퓨터 사이의 통신, 데이터 동기화 등 문제 => 텐서플로우같은 프레임워크가 해결

### 연산 정밀도와 비트 줄이기
- 메모리 용량, 버스 대역폭 등 고려
	- 메모리 용량
		- 대량의 가중치 매개변수와 중간 데이터를 메모리에 저장해야 함
	- 버스 대역폭
		- GPU(or CPU)의 버스를 흐르는 데이터가 많아져서 한계를 넘으면 병목됨
- 따라서 네트워크로 주고받는 데이터의 비트 수를 최소화해야 함
	- 많은 비트를 사용할수록 계산 오차 줄어듬, 하지만 계산에 드는 메모리 비용과 버스 대역폭에 부담
	- 딥러닝은 높은 수치 정밀도(수치를 몇비트로 하느냐)는 중요 X
		- `견고성`: 신경망 입력에 노이즈가 조금 있더라도 출력 결과가 달라지지 않음
		- 이런 견고성 때문에 신경망에 흐르는 데이터를 퇴화시켜도 됨
	- 16비트 반정밀도(half-precision)만 사용해도 학습에 문제 없다고 알려짐
