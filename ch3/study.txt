# 활성화함수 (시그모이드 vs 계단함수)
시그모이드 : 연속 -> 신경망 학습에서 중요한 역할
계단 : 불연속(0, 1)

** 모양은 같음 (입력이 작을수록 출력이 0에 가깝고 크면 1에 가까움)
** 입력이 아무리 크거나 작아도 출력은 0~1사이
	여기서 입력 : b + w1x1 + w2x2 + ....
** 둘다 비선형함수 (non-linear)
*** 신경망에서는 선형함수 쓰면 안됨 (선형함수를 이용하면 신경망 층 깊게 하는 의미가 없음, 은닉층이 없기 때문)


